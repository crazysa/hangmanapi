{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49633ae9",
   "metadata": {
    "id": "49633ae9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbcaf3",
   "metadata": {
    "id": "85fbcaf3"
   },
   "outputs": [],
   "source": [
    "def build_dictionary(dictionary_file_location):\n",
    "    text_file = open(dictionary_file_location,\"r\")\n",
    "    full_dictionary = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    return full_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa95f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1693729316671,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "25fa95f4",
    "outputId": "5b52f7da-5ee0-4ff4-e25a-beac421bbaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227300 <class 'list'> ['aaa', 'aaaaaa', 'aaas', 'aachen', 'aaee', 'aag', 'aahed', 'aahs', 'aal', 'aalesund', 'aaliis', 'aalst', 'aam', 'aandahl', 'aao', 'aapss', 'aar', 'aarau', 'aardvark', 'aardwolf', 'aaren', 'aargh', 'aarika', 'aaronic', 'aaronite', 'aaronsburg', 'aarp', 'aarrghh', 'aas', 'aasvogels', 'aaup', 'aavso', 'aba', 'ababdeh', 'abac', 'abacas', 'abacaxi', 'abaci', 'abacination', 'abaciscus', 'aback', 'abaco', 'abacterial', 'abactinally', 'abactor', 'abaculus', 'abacuses', 'abada', 'abaddon', 'abadengo', 'abadite', 'abaft', 'abagail', 'abailard', 'abaised', 'abaisse', 'abaka', 'abakas', 'abalation', 'abalienated', 'abalienation', 'abalones', 'abamp', 'abamperes', 'abana', 'abandon', 'abandoned', 'abandonee', 'abandoners', 'abandonment', 'abandons', 'abanet', 'abanic', 'abantes', 'abaptiston', 'abarambo', 'abaris', 'abarticular', 'abas', 'abased', 'abasedness', 'abasements', 'abasers', 'abasgi', 'abashed', 'abashedness', 'abashing', 'abashlessly', 'abashments', 'abasias', 'abasing', 'abask', 'abassieh', 'abastard', 'abastral', 'abatage', 'abated', 'abatements', 'abaters', 'abatic', 'abatis', 'abatises', 'abatjours', 'abator', 'abats', 'abattis', 'abattises', 'abattoirs', 'abattue', 'abature', 'abave', 'abaxile', 'abayah', 'abb', 'abbacies', 'abbacy', 'abbai', 'abbas', 'abbasid', 'abbassid', 'abbate', 'abbatical', 'abbaye', 'abbes', 'abbesses', 'abbevilean', 'abbevillian', 'abbeys', 'abbeystede', 'abbie', 'abbogada', 'abbotcies', 'abbotnullius', 'abbots', 'abbotsford', 'abbotships', 'abbotsun', 'abbottson', 'abboud', 'abbr', 'abbreviatable', 'abbreviated', 'abbreviates', 'abbreviation', 'abbreviator', 'abbreviatory', 'abbroachment', 'abbye', 'abc', 'abcissa', 'abcs', 'abdal', 'abdaria', 'abdel', 'abderhalden', 'abderite', 'abdest', 'abdicable', 'abdicate', 'abdicates', 'abdication', 'abdicative', 'abdiel', 'abditory', 'abdomen', 'abdomina', 'abdominales', 'abdominalian', 'abdominals', 'abdominocardiac', 'abdominocystic', 'abdominohysterectomy', 'abdominoposterior', 'abdominoscopy', 'abdominous', 'abdominovaginal', 'abdon', 'abduce', 'abducens', 'abducentes', 'abducing', 'abducted', 'abduction', 'abductor', 'abductors', 'abdul', 'abdulbaha', 'abe', 'abear', 'abebi', 'abecedaria', 'abecedarians', 'abecedarium', 'abecedary', 'abede', 'abednego', 'abeigh', 'abelard', 'abeles', 'abelian']\n"
     ]
    }
   ],
   "source": [
    "full_dictionary_location = \"words_250000_train.txt\"\n",
    "full_dictionary = build_dictionary(full_dictionary_location)\n",
    "print(len(full_dictionary), type(full_dictionary), full_dictionary[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b3b84",
   "metadata": {
    "id": "7d8b3b84"
   },
   "outputs": [],
   "source": [
    "chars = set(\"abcdefghijklmnopqrstuvwxyz.$\")\n",
    "characterset = set('abcdefghijklmnopqrstuvwxyz')\n",
    "char_to_int = {char: i for i, char in enumerate(sorted(chars))}\n",
    "int_to_char = {i: char for char, i in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d79fef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84358,
     "status": "ok",
     "timestamp": 1693729401023,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "a0d79fef",
    "outputId": "b4b82e1b-e2e3-4212-b9b0-d5b6d9ccd144"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d49dc1a483d0>:17: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  wrong_guess.append(random.sample(characterset-set(word), num_of_wrong_guess))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...', '......', '...s', 'aaa.', '...s', 'aaa.', '....e.', '..c...', '...h..', 'aa....'] ['aaa', 'aaaaaa', 'aaas', 'aaas', 'aaas', 'aaas', 'aachen', 'aachen', 'aachen', 'aachen'] 9188218\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_train_wrong_guesses = []\n",
    "wrong_guess = []\n",
    "for word in full_dictionary:\n",
    "    word_length = len(word)\n",
    "    unique_chars=set(word)\n",
    "    unique_chars_length = len(set(word))\n",
    "    for i in [1, 2, unique_chars_length-1]:\n",
    "      if i < unique_chars_length:\n",
    "        comb = list(combinations(unique_chars, i))\n",
    "\n",
    "        for value in comb:\n",
    "          temp_word=\"\"\n",
    "          num_of_wrong_guess = random.randint(0, 5)\n",
    "          wrong_guess.append(random.sample(characterset-set(word), num_of_wrong_guess))\n",
    "          for char in word:\n",
    "            if char  in value:\n",
    "              temp_word += char\n",
    "            else:\n",
    "              temp_word += '.'\n",
    "\n",
    "          x_train.append(temp_word)\n",
    "          y_train.append(word)\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenized_data = []\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.config.max_length = 50\n",
    "\n",
    "# for input_text, target_letter in train_data:\n",
    "#     input_ids = tokenizer.encode(input_text, add_special_tokens=False)  # Encode input text\n",
    "#     target_id = tokenizer.convert_tokens_to_ids(target_letter)  # Convert target letter to token ID\n",
    "    # tokenized_data.append({\"input_ids\": input_ids, \"target_id\": target_id})\n",
    "\n",
    "        # print(\"length of xtrain\", len(x_train), i, len(comb))\n",
    "\n",
    "      # for i in set(word):\n",
    "      #   temp_word = ''\n",
    "      #   num_of_wrong_guess = random.randint(0, 5)\n",
    "      #   wrong_guess.append(random.sample(characterset-set(word), num_of_wrong_guess))\n",
    "      #   for j in word:\n",
    "      #     if j != i:\n",
    "      #       temp_word += j\n",
    "      #     else:\n",
    "      #       temp_word += '.'\n",
    "      #   x_train.append(temp_word)\n",
    "      #   y_train.append(word)\n",
    "\n",
    "\n",
    "\n",
    "      # iterations = word_length\n",
    "      # for _ in range(iterations):\n",
    "      #     num_of_characters_to_reveal = random.randint(1, len(word)-1) if len(word)-1 >0 else random.randint(0, len(word))\n",
    "      #     num_of_wrong_guess = random.randint(0, 5)\n",
    "      #     wrong_guess.append(random.sample(characterset-set(word), num_of_wrong_guess))\n",
    "      #     chars_to_reveal = random.sample(set(word), num_of_characters_to_reveal)\n",
    "      #     temp_word = \"\"\n",
    "      #     shown_word = chars_to_reveal\n",
    "      #     for i in range(len(word)):\n",
    "      #         if  word[i] not in shown_word:\n",
    "      #             temp_word = temp_word+\".\"\n",
    "      #         else:\n",
    "      #             temp_word = temp_word+word[i]\n",
    "\n",
    "      #     x_train.append(temp_word)\n",
    "      #     y_train.append(word)\n",
    "print(x_train[:10], y_train[:10], len(wrong_guess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371eab27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37001,
     "status": "ok",
     "timestamp": 1693729438011,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "371eab27",
    "outputId": "5fcc6e8a-7847-4fa0-9daa-9e5a5288af17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9188218 tesjnajnd [[2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 20], [2, 2, 2, 20], [2, 2, 2, 20], [2, 2, 2, 20], [2, 2, 4, 9, 6, 15], [2, 2, 4, 9, 6, 15], [2, 2, 4, 9, 6, 15], [2, 2, 4, 9, 6, 15]]\n"
     ]
    }
   ],
   "source": [
    "x_train_encoded = [[char_to_int[char] for char in seq] for seq in x_train]\n",
    "y_train_encoded = [[char_to_int[char] for char in seq] for seq in y_train]\n",
    "\n",
    "x_train_encoded_wrong_guess = [[char_to_int[char] for char in seq] for seq in wrong_guess]\n",
    "print(len(x_train_encoded),\"tesjnajnd\", y_train_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b77550",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112449,
     "status": "ok",
     "timestamp": 1693729550426,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "68b77550",
    "outputId": "ed54d625-8f12-42bd-c3aa-c374884c90ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9188218, 5)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 45\n",
    "x_train_padded = np.array([seq + [char_to_int[\"$\"]] * (max_seq_length - len(seq)) for seq in x_train_encoded])\n",
    "y_train_padded = np.array([seq + [char_to_int[\"$\"]] * (50 - len(seq)) for seq in y_train_encoded])\n",
    "\n",
    "\n",
    "max_wrong_guess_length = 5\n",
    "x_train_wrong_guess_padded =np.array([seq + [char_to_int[\"$\"]] * (max_wrong_guess_length - len(seq)) for seq in x_train_encoded_wrong_guess])\n",
    "print(x_train_wrong_guess_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec9250",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1693729551240,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "09ec9250",
    "outputId": "a2b8cb88-9888-4753-a161-032a16434003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9188218, 50)\n",
      "(9188218, 50)\n"
     ]
    }
   ],
   "source": [
    "final_x_train_padded = np.concatenate((x_train_padded,x_train_wrong_guess_padded ), axis=1)\n",
    "print(final_x_train_padded.shape)\n",
    "print(y_train_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d048e3",
   "metadata": {
    "id": "21d048e3"
   },
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 55  # Dimension of character embeddings\n",
    "hidden_units = 5000  # Number of units in the LSTM layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117c956",
   "metadata": {
    "id": "0117c956"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_seq_length + max_wrong_guess_length),  # Adjust input_length\n",
    "    Bidirectional(LSTM(hidden_units, return_sequences=True)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edea02",
   "metadata": {
    "id": "24edea02"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f33118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6319832,
     "status": "ok",
     "timestamp": 1693736072886,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "26f33118",
    "outputId": "4eabe8aa-a881-4078-b3eb-05a8cd433718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17946/17946 [==============================] - 6309s 351ms/step - loss: 0.3242 - accuracy: 0.8967\n"
     ]
    }
   ],
   "source": [
    "model.fit(final_x_train_padded, y_train_padded, epochs=1, batch_size=512)\n",
    "# model.fit(data_generator, epochs=10)\n",
    "model.save(\"hangman_trained_model_1_2_single_character_highfit_bidirectinal.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b6ec8",
   "metadata": {
    "id": "410b6ec8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voqJj9sTqwmh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1693736073692,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "voqJj9sTqwmh",
    "outputId": "57877c6a-d238-421e-be0d-59658e879c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 711ms/step\n",
      "['.', '.', '.', '.', '.', 'a', '.', '.', 'a', '.', '.', '.', 'a', '.', '.', '.', '.', 'e']\n",
      "while running\n",
      "0\n",
      "while running\n",
      "1\n",
      "while running\n",
      "while running\n",
      "while running\n",
      "4\n",
      "while running\n",
      "while running\n",
      "while running\n",
      "while running\n",
      "10\n",
      "while running\n",
      "while running\n",
      "while running\n",
      "while running\n",
      "15\n",
      "while running\n",
      "16\n",
      "selected character is n\n"
     ]
    }
   ],
   "source": [
    "x_test = '.....a..a...a....'\n",
    "guessed_chars = 'e'\n",
    "x_test_encoded = [char_to_int[char] for char in x_test]\n",
    "guessed_chars_encoded = [char_to_int[char] for char in guessed_chars]\n",
    "# guessed_chars_encoded = []\n",
    "x_test_padded = np.array([x_test_encoded + [char_to_int[\"$\"]] * (45 - len(x_test_encoded))])\n",
    "guessed_chars_padded = np.array([guessed_chars_encoded + [char_to_int[\"$\"]] * (5 - len(guessed_chars_encoded))])\n",
    "# print(x_test_padded.shape, guessed_chars_padded.shape)\n",
    "final_padded = np.concatenate((x_test_padded, guessed_chars_padded), axis=1)\n",
    "predicted_probs = model.predict(final_padded)[0]\n",
    "\n",
    "\n",
    "already_selected_values = list(x_test)\n",
    "already_selected_values.extend(list(guessed_chars))\n",
    "\n",
    "print(already_selected_values)\n",
    "\n",
    "character_selected = ''\n",
    "probability_of_selected_value = 0\n",
    "for index, value in enumerate(x_test):\n",
    "  if value == '.':\n",
    "\n",
    "    # if int_to_char[np.argmax(predicted_probs[index])] not in already_selected_values:\n",
    "    #   if probability_of_selected_value < np.max(predicted_probs[index]):\n",
    "    #     character_selected = int_to_char[np.argmax(predicted_probs[index])]\n",
    "    #     probability_of_selected_value= np.max(predicted_probs[index])\n",
    "\n",
    "    # else:\n",
    "    while(True):\n",
    "      print(\"while running\")\n",
    "      if int_to_char[np.argmax(predicted_probs[index])] in already_selected_values:\n",
    "        print(\"changing\",int_to_char[np.argmax(predicted_probs[index])])\n",
    "        predicted_probs[index][np.argmax(predicted_probs[index])] = 0\n",
    "      else:\n",
    "        if probability_of_selected_value < np.max(predicted_probs[index]):\n",
    "          character_selected = int_to_char[np.argmax(predicted_probs[index])]\n",
    "          probability_of_selected_value= np.max(predicted_probs[index])\n",
    "          print(index)\n",
    "        break\n",
    "\n",
    "print(\"selected character is\",character_selected)\n",
    "\n",
    "\n",
    "\n",
    "# print(predicted_probs.shape)\n",
    "# print(int_to_char[np.argmax(predicted_probs[1])])\n",
    "# predicted_indices = np.argmax(predicted_probs, axis= 1)\n",
    "# characters_predicted=[int_to_char[index] for index in predicted_indices]\n",
    "# print(characters_predicted)\n",
    "# max_predicted_prob_for_each_axis = np.max(predicted_probs, axis=1)\n",
    "# print(max_predicted_prob_for_each_axis)\n",
    "\n",
    "# character_predicted = 'a'\n",
    "# prob_till_now = 0\n",
    "# for index,value in enumerate(characters_predicted[:len(x_test)]):\n",
    "#     if max_predicted_prob_for_each_axis[index] > prob_till_now and value in characterset and value not in guessed_chars and value not in x_test:\n",
    "#       character_predicted = value\n",
    "# print(\"character_predicted_end\", character_predicted)\n",
    "\n",
    "# print(\"predicted_probs\", predicted_probs)\n",
    "# predicted_probs = predicted_probs[0]\n",
    "# print(np.argmax(predicted_probs))\n",
    "# predicted_next_letter = int_to_char[np.argmax(predicted_probs)]\n",
    "# print(predicted_next_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OgrTeC9AZcp7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28910,
     "status": "ok",
     "timestamp": 1693736102600,
     "user": {
      "displayName": "S A",
      "userId": "04920161430718475836"
     },
     "user_tz": -330
    },
    "id": "OgrTeC9AZcp7",
    "outputId": "cda39045-90f1-48e7-84db-24c3c87f0426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1fKx7lfkPUJAdIzOv5OAz1v7D5i3hB9ku",
     "timestamp": 1693482399849
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
